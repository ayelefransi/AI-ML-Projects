{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to TensorFlow Data Validation\n",
    "\n",
    "\n",
    "## Learning objectives\n",
    "\n",
    "1. Review TFDV methods.\n",
    "2. Generate statistics.\n",
    "3. Visualize statistics.\n",
    "4. Infer a schema.\n",
    "5. Update a schema.\n",
    "\n",
    "\n",
    "\n",
    "## Introduction \n",
    "This lab is an introduction to TensorFlow Data Validation (TFDV), a key component of TensorFlow Extended.  This lab serves as a foundation for understanding the features of TFDV and how it can help you understand, validate, and monitor your data. \n",
    "\n",
    "TFDV can be used for generating schemas and statistics about the distribution of every feature in the dataset. Such information is useful for comparing multiple datasets (e.g. training vs inference datasets) and reporting:\n",
    "\n",
    "Statistical differences in the features distribution\n",
    "TFDV also offers visualization capabilities for comparing datasets based on the Google PAIR Facets project.  \n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in the [student lab notebook](../labs/tfdv_basic_spending.ipynb) -- try to complete that notebook first before reviewing this solution notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsHg6SD2nO1v"
   },
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.10/site-packages (20.0.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (2.1.3)\n",
      "Collecting tensorflow-data-validation\n",
      "  Downloading tensorflow_data_validation-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Collecting absl-py<2.0.0,>=0.9 (from tensorflow-data-validation)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-data-validation) (1.5.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow-data-validation) (2.1.3)\n",
      "Collecting pandas<2,>=1.0 (from tensorflow-data-validation)\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting pyarrow<11,>=10 (from tensorflow-data-validation)\n",
      "  Downloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting pyfarmhash<0.4,>=0.2.2 (from tensorflow-data-validation)\n",
      "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six<2,>=1.12 in /opt/conda/lib/python3.10/site-packages (from tensorflow-data-validation) (1.17.0)\n",
      "Collecting tensorflow<2.17,>=2.16 (from tensorflow-data-validation)\n",
      "  Downloading tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting tensorflow-metadata<1.17,>=1.16.0 (from tensorflow-data-validation)\n",
      "  Downloading tensorflow_metadata-1.16.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tfx-bsl<1.17,>=1.16.0 (from tensorflow-data-validation)\n",
      "  Downloading tfx_bsl-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting apache-beam<3,>=2.47 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading apache_beam-2.67.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: protobuf<5,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow-data-validation) (3.20.3)\n",
      "Collecting crcmod<2.0,>=1.7 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting orjson<4,>=3.9.7 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading orjson-3.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting dill<0.3.2,>=0.3.1.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fastavro<2,>=0.23.6 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading fastavro-1.12.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading fasteners-0.20-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting grpcio!=1.48.0,!=1.59.*,!=1.60.*,!=1.61.*,!=1.62.0,!=1.62.1,<1.66.0,<2,>=1.33.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading grpcio-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading hdfs-2.7.3.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: httplib2<0.23.0,>=0.8 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.22.0)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (4.24.0)\n",
      "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading objsize-0.7.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (25.0)\n",
      "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading pymongo-4.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.26.1)\n",
      "Collecting pydot<2,>=1.2.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading pydot-1.4.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2025.2)\n",
      "Collecting redis<6,>=5.0.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading redis-5.3.1-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting regex>=2020.6.8 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.32.4 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.32.4)\n",
      "Collecting sortedcontainers>=2.4.0 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (4.14.1)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.23.0)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /opt/conda/lib/python3.10/site-packages (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (6.0.2)\n",
      "Collecting pymilvus<3.0.0,>=2.5.10 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading pymilvus-2.6.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting pyarrow-hotfix<1 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: cachetools<7,>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (5.5.2)\n",
      "Requirement already satisfied: google-api-core<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.25.1)\n",
      "Collecting google-apitools<0.5.32,>=0.5.31 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: google-auth<3,>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.40.3)\n",
      "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.2.0)\n",
      "Collecting google-cloud-datastore<3,>=2.0.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google_cloud_datastore-2.21.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting google-cloud-pubsub<3,>=2.1.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google_cloud_pubsub-2.31.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google_cloud_pubsublite-1.12.0-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.18.2 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.19.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (3.34.0)\n",
      "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.32.0)\n",
      "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.4.3)\n",
      "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google_cloud_bigtable-2.32.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-cloud-spanner<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google_cloud_spanner-3.57.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google_cloud_dlp-3.31.0-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: google-cloud-language<3,>=2.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.17.2)\n",
      "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google_cloud_videointelligence-2.16.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google_cloud_vision-3.10.2-py3-none-any.whl.metadata (9.6 kB)\n",
      "Collecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading google_cloud_recommendations_ai-0.10.18-py3-none-any.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.103.0)\n",
      "Requirement already satisfied: keyrings.google-artifactregistry-auth in /opt/conda/lib/python3.10/site-packages (from apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.1.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3,>=2.0.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.70.0)\n",
      "Requirement already satisfied: oauth2client>=1.4.12 in /opt/conda/lib/python3.10/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (4.1.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (4.9.1)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.14.2)\n",
      "Requirement already satisfied: shapely<3.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.1.1)\n",
      "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.25.0)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.11.7)\n",
      "Requirement already satisfied: docstring_parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.16)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.48.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.7.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.14.2)\n",
      "Requirement already satisfied: google-crc32c<2.0.0dev,>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.7.1)\n",
      "Requirement already satisfied: opentelemetry-api>=1.27.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.27.0)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.27.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.27.0)\n",
      "Requirement already satisfied: overrides<8.0.0,>=6.0.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (7.7.0)\n",
      "Requirement already satisfied: sqlparse>=0.4.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.5.3)\n",
      "Collecting grpc-interceptor>=0.15.4 (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading grpc_interceptor-0.15.4-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (4.9.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.28.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (8.5.0)\n",
      "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (15.0.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.3.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.3.1)\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/lib/python3.10/site-packages (from httplib2<0.23.0,>=0.8->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (3.2.3)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.26.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.4.1)\n",
      "Requirement already satisfied: setuptools>69 in /opt/conda/lib/python3.10/site-packages (from pymilvus<3.0.0,>=2.5.10->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (80.1.0)\n",
      "INFO: pip is looking at multiple versions of pymilvus to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting pymilvus<3.0.0,>=2.5.10 (from apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading pymilvus-2.5.14-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from pymilvus<3.0.0,>=2.5.10->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.1.1)\n",
      "Collecting ujson>=2.0.0 (from pymilvus<3.0.0,>=2.5.10->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
      "Collecting milvus-lite>=2.4.0 (from pymilvus<3.0.0,>=2.5.10->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl.metadata (10.0 kB)\n",
      "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo<5.0.0,>=3.8.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting PyJWT>=2.9.0 (from redis<6,>=5.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: async-timeout>=4.0.3 in /opt/conda/lib/python3.10/site-packages (from redis<6,>=5.0.0->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (5.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.4->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.32.4->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.26.20)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.10/site-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.6.1)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=3.10.0 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting ml-dtypes~=0.3.1 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16->tensorflow-data-validation) (1.17.2)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.0.0 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading keras-3.11.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.22.0 (from tensorflow-data-validation)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting google-api-python-client<2,>=1.7.11 (from tfx-bsl<1.17,>=1.16.0->tensorflow-data-validation)\n",
      "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.17,>=1.16.0->tensorflow-data-validation)\n",
      "  Downloading tensorflow_serving_api-2.19.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.17,>=1.16.0->tensorflow-data-validation)\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow-serving-api to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.17,>=1.16.0->tensorflow-data-validation)\n",
      "  Downloading tensorflow_serving_api-2.18.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading tensorflow_serving_api-2.18.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading tensorflow_serving_api-2.17.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading tensorflow_serving_api-2.17.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "  Downloading tensorflow_serving_api-2.16.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16->tensorflow-data-validation) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow-data-validation) (13.9.4)\n",
      "Collecting namex (from keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow-data-validation)\n",
      "  Downloading optree-0.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (33 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from milvus-lite>=2.4.0->pymilvus<3.0.0,>=2.5.10->apache-beam<3,>=2.47->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (4.67.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<=8.4.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (8.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<=8.4.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.48b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.48b0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16->tensorflow-data-validation) (3.0.2)\n",
      "Requirement already satisfied: keyring in /opt/conda/lib/python3.10/site-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (25.6.0)\n",
      "Requirement already satisfied: pluggy in /opt/conda/lib/python3.10/site-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.5.0)\n",
      "Requirement already satisfied: SecretStorage>=3.2 in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (3.3.3)\n",
      "Requirement already satisfied: jeepney>=0.4.2 in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (0.9.0)\n",
      "Requirement already satisfied: jaraco.classes in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (3.4.0)\n",
      "Requirement already satisfied: jaraco.functools in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (4.2.1)\n",
      "Requirement already satisfied: jaraco.context in /opt/conda/lib/python3.10/site-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (6.0.1)\n",
      "Requirement already satisfied: cryptography>=2.0 in /opt/conda/lib/python3.10/site-packages (from SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (45.0.5)\n",
      "Requirement already satisfied: cffi>=1.14 in /opt/conda/lib/python3.10/site-packages (from cryptography>=2.0->SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.17.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.14->cryptography>=2.0->SecretStorage>=3.2->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (2.22)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/lib/python3.10/site-packages (from jaraco.classes->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (10.7.0)\n",
      "Requirement already satisfied: backports.tarfile in /opt/conda/lib/python3.10/site-packages (from jaraco.context->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.47; python_version < \"3.11\"->tensorflow-data-validation) (1.2.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow-data-validation) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow-data-validation) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow<2.17,>=2.16->tensorflow-data-validation) (0.1.2)\n",
      "Downloading tensorflow_data_validation-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Downloading apache_beam-2.67.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m152.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fastavro-1.12.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m152.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fasteners-0.20-py3-none-any.whl (18 kB)\n",
      "Downloading google_cloud_bigtable-2.32.0-py3-none-any.whl (520 kB)\n",
      "Downloading google_cloud_datastore-2.21.0-py2.py3-none-any.whl (208 kB)\n",
      "Downloading google_cloud_dlp-3.31.0-py3-none-any.whl (215 kB)\n",
      "Downloading google_cloud_pubsub-2.31.1-py3-none-any.whl (319 kB)\n",
      "Downloading google_cloud_pubsublite-1.12.0-py2.py3-none-any.whl (322 kB)\n",
      "Downloading google_cloud_recommendations_ai-0.10.18-py3-none-any.whl (212 kB)\n",
      "Downloading google_cloud_spanner-3.57.0-py3-none-any.whl (501 kB)\n",
      "Downloading google_cloud_videointelligence-2.16.2-py3-none-any.whl (275 kB)\n",
      "Downloading google_cloud_vision-3.10.2-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.9/527.9 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.65.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m127.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)\n",
      "Downloading objsize-0.7.1-py3-none-any.whl (11 kB)\n",
      "Downloading orjson-3.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-10.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
      "Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
      "Downloading pymilvus-2.5.14-py3-none-any.whl (236 kB)\n",
      "Downloading pymongo-4.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
      "Downloading redis-5.3.1-py3-none-any.whl (272 kB)\n",
      "Downloading tensorflow-2.16.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (590.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m590.6/590.6 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m157.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m149.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_metadata-1.16.1-py3-none-any.whl (28 kB)\n",
      "Downloading tfx_bsl-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
      "Downloading tensorflow_serving_api-2.16.1-py2.py3-none-any.whl (26 kB)\n",
      "Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpc_interceptor-0.15.4-py3-none-any.whl (20 kB)\n",
      "Downloading h5py-3.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m112.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.11.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m200.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Downloading milvus_lite-2.5.1-py3-none-manylinux2014_x86_64.whl (55.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 MB\u001b[0m \u001b[31m136.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Downloading regex-2025.7.34-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (789 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m789.8/789.8 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.37.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m131.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (387 kB)\n",
      "Building wheels for collected packages: crcmod, dill, google-apitools, hdfs, pyfarmhash, docopt\n",
      "\u001b[33m  DEPRECATION: Building 'crcmod' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'crcmod'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for crcmod (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for crcmod: filename=crcmod-1.7-cp310-cp310-linux_x86_64.whl size=23168 sha256=70362df0fc9e81206438490291365f889a36ac81a570c6e555531d45c1f475cc\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/85/4c/07/72215c529bd59d67e3dac29711d7aba1b692f543c808ba9e86\n",
      "\u001b[33m  DEPRECATION: Building 'dill' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'dill'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78633 sha256=39246130d71f581ae9e6a3c2745f4dc5b0f5e1544e9fbb567e4ba68d31a69799\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/ea/e2/86/64980d90e297e7bf2ce588c2b96e818f5399c515c4bb8a7e4f\n",
      "\u001b[33m  DEPRECATION: Building 'google-apitools' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'google-apitools'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for google-apitools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131076 sha256=67a1471a6279b310bee7d70b3845d17acfab3b216fe1cf4c0fa06c863b5d8f4d\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/04/b7/e0/9712f8c23a5da3d9d16fb88216b897bf60e85b12f5470f26ee\n",
      "\u001b[33m  DEPRECATION: Building 'hdfs' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'hdfs'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for hdfs (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for hdfs: filename=hdfs-2.7.3-py3-none-any.whl size=34431 sha256=8b1da20ceabaceeb7d23c184489eddb70d3b4d9f0d1214290acf6a69349c86b1\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/e5/8d/b6/99c1c0a3ac5788c866b0ecd3f48b0134a5910e6ed26011800b\n",
      "\u001b[33m  DEPRECATION: Building 'pyfarmhash' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pyfarmhash'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for pyfarmhash (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp310-cp310-linux_x86_64.whl size=13938 sha256=7ac3c5504741c62cc0d2674ea6772ad3f01de93a8d2a7f7dd101369e614a7528\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/e0/08/da/f66b1f3258fe3f1e767b2136c5444dbfa9fa3f7944cc5e1983\n",
      "\u001b[33m  DEPRECATION: Building 'docopt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docopt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13781 sha256=9727c040e609c23fc42fc33071ee14e1e5ebb324c77386e13e2fb4b5d6217d05\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
      "Successfully built crcmod dill google-apitools hdfs pyfarmhash docopt\n",
      "Installing collected packages: sortedcontainers, pyfarmhash, namex, libclang, flatbuffers, docopt, crcmod, werkzeug, uritemplate, ujson, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, regex, PyJWT, pydot, pyarrow-hotfix, orjson, optree, opt-einsum, objsize, numpy, milvus-lite, markdown, jsonpickle, grpcio, google-pasta, gast, fasteners, fastavro, dnspython, dill, astunparse, absl-py, tensorflow-metadata, tensorboard, redis, pymongo, pyarrow, pandas, ml-dtypes, hdfs, h5py, grpc-interceptor, pymilvus, keras, google-apitools, tensorflow, google-api-python-client, apache-beam, tensorflow-serving-api, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-cloud-pubsublite, tfx-bsl, tensorflow-data-validation\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/62\u001b[0m [numpy]nsum]-data-server]stem]\u001b[33m  WARNING: The script f2py is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22/62\u001b[0m [milvus-lite]\u001b[33m  WARNING: The script milvus-lite is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: The script markdown_py is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28/62\u001b[0m [fasteners]]\u001b[33m  WARNING: The script fastavro is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35/62\u001b[0m [tensorboard]\u001b[33m  WARNING: The script tensorboard is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38/62\u001b[0m [pyarrow]\u001b[33m  WARNING: The script plasma_store is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m41/62\u001b[0m [hdfs]ypes]\u001b[33m  WARNING: The scripts hdfscli and hdfscli-avro are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━\u001b[0m \u001b[32m45/62\u001b[0m [keras]us]\u001b[33m  WARNING: The script gen_client is installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m47/62\u001b[0m [tensorflow]\u001b[33m  WARNING: The scripts import_pb_to_tensorboard, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/home/jupyter/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62/62\u001b[0m [tensorflow-data-validation]lidation]blite]s-ai]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.11.0 requires pyarrow>=15.0.2, but you have pyarrow 10.0.1 which is incompatible.\n",
      "db-dtypes 1.4.3 requires pyarrow>=13.0.0, but you have pyarrow 10.0.1 which is incompatible.\n",
      "geopandas 1.1.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "visions 0.8.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed PyJWT-2.10.1 absl-py-1.4.0 apache-beam-2.67.0 astunparse-1.6.3 crcmod-1.7 dill-0.3.1.1 dnspython-2.7.0 docopt-0.6.2 fastavro-1.12.0 fasteners-0.20 flatbuffers-25.2.10 gast-0.6.0 google-api-python-client-1.12.11 google-apitools-0.5.31 google-cloud-bigtable-2.32.0 google-cloud-datastore-2.21.0 google-cloud-dlp-3.31.0 google-cloud-pubsub-2.31.1 google-cloud-pubsublite-1.12.0 google-cloud-recommendations-ai-0.10.18 google-cloud-spanner-3.57.0 google-cloud-videointelligence-2.16.2 google-cloud-vision-3.10.2 google-pasta-0.2.0 grpc-interceptor-0.15.4 grpcio-1.65.5 h5py-3.14.0 hdfs-2.7.3 jsonpickle-3.4.2 keras-3.11.2 libclang-18.1.1 markdown-3.8.2 milvus-lite-2.5.1 ml-dtypes-0.3.2 namex-0.1.0 numpy-1.26.4 objsize-0.7.1 opt-einsum-3.4.0 optree-0.17.0 orjson-3.11.2 pandas-1.5.3 pyarrow-10.0.1 pyarrow-hotfix-0.7 pydot-1.4.2 pyfarmhash-0.3.2 pymilvus-2.5.14 pymongo-4.14.0 redis-5.3.1 regex-2025.7.34 sortedcontainers-2.4.0 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.2 tensorflow-data-validation-1.16.1 tensorflow-io-gcs-filesystem-0.37.1 tensorflow-metadata-1.16.1 tensorflow-serving-api-2.16.1 termcolor-3.1.0 tfx-bsl-1.16.1 ujson-5.10.0 uritemplate-3.0.1 werkzeug-3.1.3\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow\n",
    "!pip install numpy\n",
    "!pip install tensorflow-data-validation --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AsHg6SD2nO1v"
   },
   "source": [
    "**Restart the kernel (Kernel > Restart kernel > Restart).**\n",
    "\n",
    "**Re-run the above cell and proceed further.**\n",
    "\n",
    "**Note: Please ignore any incompatibility warnings and errors.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:12: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_data_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow_data_validation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtfdv\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_data_validation'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow_data_validation as tfdv\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print('Installing TensorFlow Data Validation')\n",
    "!pip install -q tensorflow_data_validation[visualization]\n",
    "\n",
    "print('TFDV version: {}'.format(tfdv.version.__version__))\n",
    "# Confirm that we're using Python 3\n",
    "assert sys.version_info.major is 3, 'Oops, not running Python 3. Use Runtime > Change runtime type'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fnm6Mj3vTGLm"
   },
   "source": [
    "###  Load the Consumer Spending Dataset\n",
    "\n",
    "We will download our dataset from Google Cloud Storage. The columns in the dataset are:\n",
    "\n",
    "* 'Graduated': Whether or not the person is a college graduate\n",
    "* 'Work Experience': The number of years in the workforce\n",
    "* 'Family Size': The size of the family unit\n",
    "* 'Spending Score': The spending score for consumer spending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Spending_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Engineer</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Graduated     Profession  Work_Experience  Family_Size Spending_Score\n",
       "0        No     Healthcare              1.0          4.0            Low\n",
       "1       Yes       Engineer              NaN          3.0        Average\n",
       "2       Yes       Engineer              1.0          1.0            Low\n",
       "3       Yes         Lawyer              0.0          2.0           High\n",
       "4       Yes  Entertainment              NaN          6.0           High"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "score_train = pd.read_csv('data/score_train.csv')\n",
    "score_train.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Graduated</th>\n",
       "      <th>Profession</th>\n",
       "      <th>Work_Experience</th>\n",
       "      <th>Family_Size</th>\n",
       "      <th>Spending_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>Doctor</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>Lawyer</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Executive</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yes</td>\n",
       "      <td>Artist</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Average</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Graduated     Profession  Work_Experience  Family_Size Spending_Score\n",
       "0        No         Doctor              0.0          5.0        Average\n",
       "1       Yes  Entertainment              1.0          4.0        Average\n",
       "2        No         Lawyer              0.0          5.0            Low\n",
       "3       Yes      Executive              1.0          5.0           High\n",
       "4       Yes         Artist              1.0          2.0        Average"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO\n",
    "score_test = pd.read_csv('data/score_test.csv')\n",
    "score_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Graduated        3964 non-null   object \n",
      " 1   Profession       3944 non-null   object \n",
      " 2   Work_Experience  3589 non-null   float64\n",
      " 3   Family_Size      3831 non-null   float64\n",
      " 4   Spending_Score   4000 non-null   object \n",
      "dtypes: float64(2), object(3)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "score_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the methods present in TFDV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CombinerStatsGenerator',\n",
       " 'CrossFeatureView',\n",
       " 'DatasetListView',\n",
       " 'DatasetView',\n",
       " 'DetectFeatureSkew',\n",
       " 'FeaturePath',\n",
       " 'FeatureView',\n",
       " 'GenerateStatistics',\n",
       " 'MergeDatasetFeatureStatisticsList',\n",
       " 'StatsOptions',\n",
       " 'TransformStatsGenerator',\n",
       " 'WriteStatisticsToBinaryFile',\n",
       " 'WriteStatisticsToRecordsAndBinaryFile',\n",
       " 'WriteStatisticsToTFRecord',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " 'anomalies',\n",
       " 'api',\n",
       " 'arrow',\n",
       " 'coders',\n",
       " 'compare_slices',\n",
       " 'constants',\n",
       " 'default_sharded_output_suffix',\n",
       " 'default_sharded_output_supported',\n",
       " 'display_anomalies',\n",
       " 'display_schema',\n",
       " 'experimental_get_feature_value_slicer',\n",
       " 'generate_dummy_schema_with_paths',\n",
       " 'generate_statistics_from_csv',\n",
       " 'generate_statistics_from_dataframe',\n",
       " 'generate_statistics_from_tfrecord',\n",
       " 'get_confusion_count_dataframes',\n",
       " 'get_domain',\n",
       " 'get_feature',\n",
       " 'get_feature_stats',\n",
       " 'get_match_stats_dataframe',\n",
       " 'get_skew_result_dataframe',\n",
       " 'get_slice_stats',\n",
       " 'get_statistics_html',\n",
       " 'infer_schema',\n",
       " 'load_anomalies_text',\n",
       " 'load_schema_text',\n",
       " 'load_sharded_statistics',\n",
       " 'load_statistics',\n",
       " 'load_stats_binary',\n",
       " 'load_stats_text',\n",
       " 'pywrap',\n",
       " 'set_domain',\n",
       " 'skew',\n",
       " 'statistics',\n",
       " 'types',\n",
       " 'update_schema',\n",
       " 'utils',\n",
       " 'validate_corresponding_slices',\n",
       " 'validate_examples_in_csv',\n",
       " 'validate_examples_in_tfrecord',\n",
       " 'validate_statistics',\n",
       " 'version',\n",
       " 'visualize_statistics',\n",
       " 'write_anomalies_text',\n",
       " 'write_schema_text',\n",
       " 'write_stats_text']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check methods present in tfdv\n",
    "# TODO\n",
    "[methods for methods in dir(tfdv)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describing data with TFDV\n",
    "The usual workflow when using TFDV during training is as follows:\n",
    "\n",
    "\n",
    "1.   Generate statistics for the data\n",
    "2.   Use those statistics to generate a schema for each feature\n",
    "3.   Visualize the schema and statistics and manually inspect them\n",
    "4.   Update the schema if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and visualize statistics\n",
    "\n",
    "First we'll use [`tfdv.generate_statistics_from_csv`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv) to compute statistics for our training data. (ignore the snappy warnings)\n",
    "\n",
    "TFDV can compute descriptive [statistics](https://github.com/tensorflow/metadata/blob/v0.6.0/tensorflow_metadata/proto/v0/statistics.proto) that provide a quick overview of the data in terms of the features that are present and the shapes of their value distributions.\n",
    "\n",
    "Internally, TFDV uses [Apache Beam](https://beam.apache.org/)'s data-parallel processing framework to scale the computation of statistics over large datasets. For applications that wish to integrate deeper with TFDV (e.g., attach statistics generation at the end of a data-generation pipeline), the API also exposes a Beam PTransform for statistics generation.\n",
    "\n",
    "**NOTE:  Compute statistics**\n",
    "* [tfdv.generate_statistics_from_csv](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_csv)\n",
    "* [tfdv.generate_statistics_from_dataframe](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_dataframe)\n",
    "* [tfdv.generate_statistics_from_tfrecord](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/generate_statistics_from_tfrecord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Statistics from a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute data statistics for the input pandas DataFrame.\n",
    "# TODO\n",
    "stats = tfdv.generate_statistics_from_dataframe(dataframe=score_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use [`tfdv.visualize_statistics`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/visualize_statistics), which uses [Facets](https://pair-code.github.io/facets/) to create a succinct visualization of our training data:\n",
    "\n",
    "* Notice that numeric features and categorical features are visualized separately, and that charts are displayed showing the distributions for each feature.\n",
    "* Notice that features with missing or zero values display a percentage in red as a visual indicator that there may be issues with examples in those features.  The percentage is the percentage of examples that have missing or zero values for that feature.\n",
    "* Notice that there are no examples with values for `pickup_census_tract`.  This is an opportunity for dimensionality reduction!\n",
    "* Try clicking \"expand\" above the charts to change the display\n",
    "* Try hovering over bars in the charts to display bucket ranges and counts\n",
    "* Try switching between the log and linear scales, and notice how the log scale reveals much more detail about the `payment_type` categorical feature\n",
    "* Try selecting \"quantiles\" from the \"Chart to show\" menu, and hover over the markers to show the quantile percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the input statistics using Facets.\n",
    "# TODO\n",
    "tfdv.visualize_statistics(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFDV generates different types of statistics based on the type of features.\n",
    "\n",
    "**For numerical features, TFDV computes for every feature:**\n",
    "* Count of records\n",
    "* Number of missing (i.e. null values)\n",
    "* Histogram of values\n",
    "* Mean and standard deviation\n",
    "* Minimum and maximum values\n",
    "* Percentage of zero values\n",
    "\n",
    "**For categorical features, TFDV provides:**\n",
    "* Count of values\n",
    "* Percentage of missing values\n",
    "* Number of unique values\n",
    "* Average string length\n",
    "* Count for each label and its rank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's compare the score_train and the score_test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id='facets-iframe' width=\"100%\" height=\"500px\"></iframe>\n",
       "        <script>\n",
       "        facets_iframe = document.getElementById('facets-iframe');\n",
       "        facets_html = '<script src=\"https://cdnjs.cloudflare.com/ajax/libs/webcomponentsjs/1.3.3/webcomponents-lite.js\"><\\/script><link rel=\"import\" href=\"https://raw.githubusercontent.com/PAIR-code/facets/master/facets-dist/facets-jupyter.html\"><facets-overview proto-input=\"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\"></facets-overview>';\n",
       "        facets_iframe.srcdoc = facets_html;\n",
       "         facets_iframe.id = \"\";\n",
       "         setTimeout(() => {\n",
       "           facets_iframe.setAttribute('height', facets_iframe.contentWindow.document.body.offsetHeight + 'px')\n",
       "         }, 1500)\n",
       "         </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_stats = tfdv.generate_statistics_from_dataframe(dataframe=score_train)\n",
    "test_stats = tfdv.generate_statistics_from_dataframe(dataframe=score_test)\n",
    "\n",
    "tfdv.visualize_statistics(\n",
    "  lhs_statistics=train_stats, lhs_name='TRAIN_DATASET',\n",
    "  rhs_statistics=test_stats, rhs_name='NEW_DATASET')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KVR02-y4V0uM"
   },
   "source": [
    "### Infer a schema\n",
    "\n",
    "Now let's use [`tfdv.infer_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema) to create a schema for our data.  A schema defines constraints for the data that are relevant for ML. Example constraints include the data type of each feature, whether it's numerical or categorical, or the frequency of its presence in the data.  For categorical features the schema also defines the domain - the list of acceptable values.  Since writing a schema can be a tedious task, especially for datasets with lots of features, TFDV provides a method to generate an initial version of the schema based on the descriptive statistics.\n",
    "\n",
    "Getting the schema right is important because the rest of our production pipeline will be relying on the schema that TFDV generates to be correct.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating Schema\n",
    "Once statistics are generated, the next step is to generate a schema for our dataset. This schema will map each feature in the dataset to a type (float, bytes, etc.). Also define feature boundaries (min, max, distribution of values and missings, etc.).\n",
    "\n",
    "Link to infer schema\n",
    "https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/infer_schema\n",
    "\n",
    "With TFDV, we generate schema from statistics using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6LLkRJThVr9m"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature {\n",
      "  name: \"Graduated\"\n",
      "  value_count {\n",
      "    min: 1\n",
      "    max: 1\n",
      "  }\n",
      "  type: BYTES\n",
      "  domain: \"Graduated\"\n",
      "  presence {\n",
      "    min_count: 1\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"Profession\"\n",
      "  value_count {\n",
      "    min: 1\n",
      "    max: 1\n",
      "  }\n",
      "  type: BYTES\n",
      "  domain: \"Profession\"\n",
      "  presence {\n",
      "    min_count: 1\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"Work_Experience\"\n",
      "  value_count {\n",
      "    min: 1\n",
      "    max: 1\n",
      "  }\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_count: 1\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"Family_Size\"\n",
      "  value_count {\n",
      "    min: 1\n",
      "    max: 1\n",
      "  }\n",
      "  type: FLOAT\n",
      "  presence {\n",
      "    min_count: 1\n",
      "  }\n",
      "}\n",
      "feature {\n",
      "  name: \"Spending_Score\"\n",
      "  type: BYTES\n",
      "  domain: \"Spending_Score\"\n",
      "  presence {\n",
      "    min_fraction: 1.0\n",
      "    min_count: 1\n",
      "  }\n",
      "  shape {\n",
      "    dim {\n",
      "      size: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "string_domain {\n",
      "  name: \"Graduated\"\n",
      "  value: \"No\"\n",
      "  value: \"Yes\"\n",
      "}\n",
      "string_domain {\n",
      "  name: \"Profession\"\n",
      "  value: \"Artist\"\n",
      "  value: \"Doctor\"\n",
      "  value: \"Engineer\"\n",
      "  value: \"Entertainment\"\n",
      "  value: \"Executive\"\n",
      "  value: \"Healthcare\"\n",
      "  value: \"Homemaker\"\n",
      "  value: \"Lawyer\"\n",
      "  value: \"Marketing\"\n",
      "}\n",
      "string_domain {\n",
      "  name: \"Spending_Score\"\n",
      "  value: \"Average\"\n",
      "  value: \"High\"\n",
      "  value: \"Low\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Infers schema from the input statistics.\n",
    "# TODO\n",
    "schema = tfdv.infer_schema(statistics=stats)\n",
    "print(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The schema also provides documentation for the data, and so is useful when different developers work on the same data.  Let's use [`tfdv.display_schema`](https://www.tensorflow.org/tfx/data_validation/api_docs/python/tfdv/display_schema) to display the inferred schema so that we can review it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Valency</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'Graduated'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>'Graduated'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Profession'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>'Profession'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Work_Experience'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Family_Size'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Spending_Score'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>'Spending_Score'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Type  Presence Valency            Domain\n",
       "Feature name                                                 \n",
       "'Graduated'        STRING  optional  single       'Graduated'\n",
       "'Profession'       STRING  optional  single      'Profession'\n",
       "'Work_Experience'   FLOAT  optional  single                 -\n",
       "'Family_Size'       FLOAT  optional  single                 -\n",
       "'Spending_Score'   STRING  required          'Spending_Score'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'Graduated'</th>\n",
       "      <td>'No', 'Yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Profession'</th>\n",
       "      <td>'Artist', 'Doctor', 'Engineer', 'Entertainment', 'Executive', 'Healthcare', 'Homemaker', 'Lawyer', 'Marketing'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Spending_Score'</th>\n",
       "      <td>'Average', 'High', 'Low'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                          Values\n",
       "Domain                                                                                                                          \n",
       "'Graduated'                                                                                                          'No', 'Yes'\n",
       "'Profession'      'Artist', 'Doctor', 'Engineer', 'Entertainment', 'Executive', 'Healthcare', 'Homemaker', 'Lawyer', 'Marketing'\n",
       "'Spending_Score'                                                                                        'Average', 'High', 'Low'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfdv.display_schema(schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFDV provides a API to print a summary of each feature schema using\n",
    "\n",
    "In this visualization, the columns stand for:\n",
    "\n",
    "**Type** indicates the feature datatype.\n",
    "\n",
    "**Presence** indicates whether the feature must be present in 100% of examples (required) or not (optional).\n",
    "\n",
    "**Valency** indicates the number of values required per training example. \n",
    "\n",
    "**Domain and Values** indicates The feature domain and its values\n",
    "\n",
    "In the case of categorical features, single indicates that each training example must have exactly one category for the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating the Schema \n",
    "As stated above, **Presence** indicates whether the feature must be present in 100% of examples (required) or not (optional).  Currently, all of our features except for our target label are shown as \"optional\". We need to make our features all required except for \"Work Experience\".  We will need to update the schema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFDV lets you update the schema according to your domain knowledge of the data if you are not satisfied by the auto-generated schema.  We will update three use cases:  Making a feature required, adding a value to a feature, and change a feature from a float to an integer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change optional features to required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Family_Size from FLOAT to Int\n",
    "Graduated_feature = tfdv.get_feature(schema, 'Graduated')\n",
    "Graduated_feature.presence.min_fraction = 1.0\n",
    "Profession_feature = tfdv.get_feature(schema, 'Profession')\n",
    "Profession_feature.presence.min_fraction = 1.0\n",
    "Family_Size_feature = tfdv.get_feature(schema, 'Family_Size')\n",
    "Family_Size_feature.presence.min_fraction = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Valency</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'Graduated'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>'Graduated'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Profession'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>'Profession'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Work_Experience'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Family_Size'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Spending_Score'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>'Spending_Score'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Type  Presence Valency            Domain\n",
       "Feature name                                                 \n",
       "'Graduated'        STRING  required  single       'Graduated'\n",
       "'Profession'       STRING  required  single      'Profession'\n",
       "'Work_Experience'   FLOAT  optional  single                 -\n",
       "'Family_Size'       FLOAT  required  single                 -\n",
       "'Spending_Score'   STRING  required          'Spending_Score'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'Graduated'</th>\n",
       "      <td>'No', 'Yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Profession'</th>\n",
       "      <td>'Artist', 'Doctor', 'Engineer', 'Entertainment', 'Executive', 'Healthcare', 'Homemaker', 'Lawyer', 'Marketing'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Spending_Score'</th>\n",
       "      <td>'Average', 'High', 'Low'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                          Values\n",
       "Domain                                                                                                                          \n",
       "'Graduated'                                                                                                          'No', 'Yes'\n",
       "'Profession'      'Artist', 'Doctor', 'Engineer', 'Entertainment', 'Executive', 'Healthcare', 'Homemaker', 'Lawyer', 'Marketing'\n",
       "'Spending_Score'                                                                                        'Average', 'High', 'Low'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update a feature with a new value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add \"self-employed\" to the Profession feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employed', 'Artist', 'Doctor', 'Engineer', 'Entertainment', 'Executive', 'Healthcare', 'Homemaker', 'Lawyer', 'Marketing']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Profession_domain = tfdv.get_domain(schema, 'Profession')\n",
    "Profession_domain.value.insert(0, 'Self-Employed')\n",
    "Profession_domain.value\n",
    "# [0 indicates I want 'Self-Employed to come first', if the number were 3, \n",
    "# it would be placed after the third value. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's remove \"Homemaker\" from \"Profession\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Profession_domain = tfdv.get_domain(schema, 'Profession')\n",
    "Profession_domain.value.remove('Homemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Self-Employed', 'Artist', 'Doctor', 'Engineer', 'Entertainment', 'Executive', 'Healthcare', 'Lawyer', 'Marketing']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Profession_domain.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change a feature from a float to an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update Family_Size to Int\n",
    "size = tfdv.get_feature(schema, 'Family_Size')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Presence</th>\n",
       "      <th>Valency</th>\n",
       "      <th>Domain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feature name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'Graduated'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>'Graduated'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Profession'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>'Profession'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Work_Experience'</th>\n",
       "      <td>FLOAT</td>\n",
       "      <td>optional</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Family_Size'</th>\n",
       "      <td>INT</td>\n",
       "      <td>required</td>\n",
       "      <td>single</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Spending_Score'</th>\n",
       "      <td>STRING</td>\n",
       "      <td>required</td>\n",
       "      <td></td>\n",
       "      <td>'Spending_Score'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Type  Presence Valency            Domain\n",
       "Feature name                                                 \n",
       "'Graduated'        STRING  required  single       'Graduated'\n",
       "'Profession'       STRING  required  single      'Profession'\n",
       "'Work_Experience'   FLOAT  optional  single                 -\n",
       "'Family_Size'         INT  required  single                 -\n",
       "'Spending_Score'   STRING  required          'Spending_Score'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Values</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Domain</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'Graduated'</th>\n",
       "      <td>'No', 'Yes'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Profession'</th>\n",
       "      <td>'Self-Employed', 'Artist', 'Doctor', 'Engineer', 'Entertainment', 'Executive', 'Healthcare', 'Lawyer', 'Marketing'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>'Spending_Score'</th>\n",
       "      <td>'Average', 'High', 'Low'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                              Values\n",
       "Domain                                                                                                                              \n",
       "'Graduated'                                                                                                              'No', 'Yes'\n",
       "'Profession'      'Self-Employed', 'Artist', 'Doctor', 'Engineer', 'Entertainment', 'Executive', 'Healthcare', 'Lawyer', 'Marketing'\n",
       "'Spending_Score'                                                                                            'Average', 'High', 'Low'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "size.type=2\n",
    "tfdv.display_schema(schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next lab, you compare two datasets and check for anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b8eC59yISdGB"
   },
   "source": [
    "## When to use TFDV\n",
    "\n",
    "It's easy to think of TFDV as only applying to the start of your training pipeline, as we did here, but in fact it has many uses. Here are a few more:\n",
    "\n",
    "* Validating new data for inference to make sure that we haven't suddenly started receiving bad features\n",
    "* Validating new data for inference to make sure that our model has trained on that part of the decision surface\n",
    "* Validating our data after we've transformed it and done feature engineering (probably using [TensorFlow Transform](https://www.tensorflow.org/tfx/guide/transform)) to make sure we haven't done something wrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/GoogleCloudPlatform/mlops-on-gcp/blob/master/examples/tfdv-structured-data/tfdv-covertype.ipynb"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "tghWegsjhpkt"
   ],
   "name": "tfdv_basic.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m131",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m131"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
